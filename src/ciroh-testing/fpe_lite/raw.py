'''
Utilities for raw NWM data access, processing and storage
'''

import pandas as pd
import numpy as np
import xarray as xr
import requests
import time
from datetime import datetime

from . import admin, files
    
def check_files_exist(data_dir, df_filelists):
    ''' 
    check that all nwm files needed for the evaluation (based on filelists)
    already exist or were successfully downloaded
        - if any channel_rt output file is missing - set flag to False which will skip the reftime evaluation
        - if forcing grids are missing, allow calculations to continue 
    '''
    
    all_files_exist = True

    for index, row in df_filelists.iterrows():
    
        filelist = row['filelist']
        ref_time = row['ref_time']
        version_dir = admin.nwm_version_dir(data_dir, ref_time)
        
        for nwm_path in filelist: 

            full_path = version_dir / nwm_path 

            if not full_path.exists():
                all_files_exist = False            
                print("     !!! NWM file ", full_path, " not downloaded")
                
            # if file is less than 1000 KB suspect, too small for subset or full file,
            # likely not fully downloaded 
            # will not catch all file issues, but some
            elif full_path.stat().st_size < 1000000:
                all_files_exist = False            
                print("     !!! NWM file ", full_path, " too small, not fully downloaded")
            
        
    return all_files_exist    
     
    
def download_nwm_by_df_filelists(data_dir, df_filelists, source_list = ['dstor','nomads','google'], cert_path = 'none', 
                                 subset = True, try_alt_ana = False):
    '''
    Extract a filelist from a dataframe of filelists (as generated by build_df_filelists) and initiate downloading
    '''
    
    # initialize filelist update if storing subsets
    if subset:
        df_filelists_updated = pd.DataFrame()
        
    # if df_filelists totally empty, abort
    if df_filelists.empty:
        raise ValueError('Filelist empty for all reference times and variables - check configuration')
     
    # iterate through the dataframe, extracting filelists (for multiple configs, variables, and/or reference times)
    for index, row in df_filelists.iterrows():
    
        # extract filelist and check if it is empty, skip if empty
        filelist = row['filelist']
        if not filelist:
            print("\nFilelist empty for: ", row['variable'], row['config'], row['ref_time'])
            continue

        # execute download function for each filelist
        print("\nDownloading nwm output for: ", row['variable'], row['config'], row['ref_time'])
        filelist = download_nwm_one_filelist(data_dir, filelist, source_list = source_list, cert_path = cert_path, 
                                             subset = subset, try_alt_ana = try_alt_ana)
        
        # store updated filelist if subset was created/stored
        if subset:
            df_filelists_updated = df_filelists_updated.append(pd.DataFrame({'filelist' : [filelist]}))

    # overwrite filelists in main dataframe if subsets were stored
    if subset:
        df_filelists['filelist'] = df_filelists_updated.loc[:,'filelist'].to_list()
    
    return df_filelists

                    
def download_nwm_one_filelist(data_dir, filelist, source_list = ['dstor','nomads','google'], cert_path = 'none', 
                              subset = True, use_local_cache = True, try_alt_ana = False):
    '''
    Download all files in a filelist from specified source(s)
    If subset = True, only the needed portion of the netcdf files will be stored locally
    '''
    
    # start timer for the full list
    t_download_start = time.time()

    # initiate lists
    file_in_cache = []
    filelist_update = filelist.copy()
    
    # if allowing use of alternate source of AnA (when main source fails), get alt_filelist
    filelist_alt = []
    is_forecast = admin.check_if_forecast_from_path(filelist[1])
    
    if try_alt_ana and not is_forecast:
        filelist_alt = files.get_alt_ana_filelist(filelist)    
    
    # loop through the filelist, try downloading each file as specified
    for i, nwm_path_main in enumerate(filelist): 
    
        if try_alt_ana and filelist_alt:
            nwm_path_list = [nwm_path_main] + [filelist_alt[i]]
        else:
            nwm_path_list = [nwm_path_main]
            
        for nwm_path in nwm_path_list:    

            # initialize status for this file
            status = "" 
            
            # get ref_time from path
            ref_time, ts, config = files.parse_nwm_path(nwm_path)
            
            # get version dir based on ref_time
            version = admin.nwm_version(ref_time)
            version_dir = admin.nwm_version_dir(data_dir, ref_time)
            
            # check if data directory exists, if not create
            netcdf_dir = version_dir / nwm_path.parent        
            admin.create_dir_if_not_exist(netcdf_dir)

            # build full output path
            full_path = netcdf_dir / nwm_path.parts[2]       
            
            if subset:
                # add subset extension to filename in path
                subset_path = get_subset_path(full_path)
                
                # replace the nwm_path in the filelist 
                filelist_update[i] = nwm_path.parent / subset_path.name
                
            elif nwm_path_list.index(nwm_path) == 1:
               # if was not subset, but trying alternate ana file, replace it in main list
               filelist_update[i] = nwm_path.parent / nwm_path.name
                
            # if nomads is one of the selected sources, check that ref_time is within the last 2 days
            # get current clock utc time (top of the last hour) 
            clock_ztime = datetime.utcnow().replace(second=0, microsecond=0, minute=0)
            if (clock_ztime - ref_time).days > 2:
                source_list = [s for s in source_list if s!='nomads']
            
            # check if requested data file already exists in local cache
            if subset and subset_path.exists() and use_local_cache:            
                print("Subset file already in cache: ", str(subset_path))    
                status = 'success'
            
            elif subset and full_path.exists() and use_local_cache:
                print("Full file in cache, replacing with subset")
                store_netcdf_subset(full_path)  
                status = 'success'                    

            elif not subset and full_path.exists() and use_local_cache:
                print("Full file already in cache: ", str(full_path))
                status = 'success'
            
            # if the requested data does not already exist - download data
            else:   
            
            # try downloading nwm data from sources listed, in order
            # only proceed to each alternate source if unsuccessful with higher choice source      

                # initialize status as failed, reset if successfully downloaded
                status = 'failed'

                # loop through specified sources, in order
                for source in source_list:
                
                    # start timer for this file
                    t_start = time.time()       
                    
                    if source == "google":
                    
                        # get UTF-8 encoded path needed for request
                        encoded_nwm_path = get_nwm_encoded_path(nwm_path) 
                        try:
                            status = get_from_google(encoded_nwm_path, full_path)
                        except:
                            print('   Connection to Google Cloud failed')
                            continue

                    elif source == "nomads": 
                        try:
                            status = get_from_nomads(nwm_path, full_path, cert_path)
                        except:
                            print('   Connection to NOMADS failed')
                            continue

                    elif source == "dstor":

                        try:
                            status = get_from_dstor(nwm_path, full_path, cert_path, version = version)
                        except:
                            print('   Connection to DSTOR failed')
                            continue
                                           
                    if status == 'success':
                        if subset:
                            store_netcdf_subset(full_path)

                        t_stop = time.time()                
                        print('   Elapsed time this file', t_stop - t_start)  
                        break
                        
            # if the first nwm_path was successful, do not try alternate
            if status == 'success':
                break
            else:
                if len(nwm_path_list) > 1:
                    print('---Trying alternate configuration:')
    
    #for path in filelist_update:
    #    print(path)
        
    t_download_end = time.time()
    print("\n---Download complete - Total download time", (t_download_end - t_download_start)/60, " min.")
            
    return filelist_update
    
    
def get_nwm_encoded_path(nwm_path):
    '''
    Create encoded path needed for download from google cloud
    '''

    # parse out nwm directories and filename from full_path
    datedir = nwm_path.parts[0]
    config_dir = nwm_path.parts[1]
    filename = nwm_path.parts[2]
    
    # rebuild with UTF-8 encoding for directory slash   
    slash = "%2F"
    encoded_path = datedir + slash + config_dir + slash + filename    
        
    return encoded_path
    

def get_from_google(encoded_nwm_path, full_path):
    '''
    Build Google Cloud URL and call download
    '''
    
    url_prefix = "https://storage.googleapis.com/download/storage/v1/b/national-water-model/o/"
    url_suffix = "?alt=media"
    url = url_prefix + encoded_nwm_path + url_suffix
    
    print("Try fetching from Google Cloud @ URL " + url)
    status = get_nwm_from_url_store_local(url, full_path, 'none', 'Google Cloud')
    
    return status  
    
    
def get_from_nomads(nwm_path, full_path, cert_path):
    '''
    Build NOMADS URL and call download
    '''
    
    url_prefix = "https://nomads.ncep.noaa.gov/pub/data/nccf/com/nwm/prod/"
    url = url_prefix + "/".join(list(nwm_path.parts))
    
    print("Try fetching from NOMADS @ URL " + url)
    status = get_nwm_from_url_store_local(url, full_path, cert_path, 'NOMADS')
    
    return status   
    
def get_from_nomads_ftp(nwm_path, full_path, cert_path):
    '''
    Build NOMADS URL and call download
    '''
    
    url_prefix = "ftp://ftpprd.ncep.noaa.gov/pub/data/nccf/com/nwm/prod/"
    url = url_prefix + "/".join(list(nwm_path.parts))
    
    print("\nTry fetching from NOMADS FTP @ URL " + url)
    status = get_nwm_from_url_store_local(url, full_path, cert_path, 'NOMADS')
    
    return status  
    
    
def get_from_dstor(nwm_path, full_path, cert_path, version = 2.2):
    '''
    Build DSTOR URL and call download
    '''    

    url_prefix = "https://nwcal-dstore.nwc.nws.noaa.gov/nwm/" + str(version) + "/"
    url = url_prefix + "/".join(list(nwm_path.parts))
    
    print("Try fetching from DSTOR @ URL " + url)
    status = get_nwm_from_url_store_local(url, full_path, cert_path, 'DSTOR')
    
    return status
    
    
def get_nwm_from_url_store_local(url, full_path, cert_path, source_str):
    '''
    Try downloading from specified URL
    '''

    if cert_path == 'none':
        response = requests.get(url, verify = True)
    else:
        response = requests.get(url, verify = cert_path)

    if response.status_code == 200:
        print("   Writing file to: ", full_path)
        with full_path.open("wb") as f:
            f.write(response.content)
        status = 'success'
    else:
        print("---Data not available on", source_str)
        status = "failed"
    
    return status 
    
    
def get_nwm_from_url_store_local_ftp(url, full_path, cert_path, source_str):
    '''
    Try downloading from specified URL
    '''

    response = urllib.request.urlretrieve(url, full_path)
    
    
    
    if cert_path == 'none':
        response = requests.get(url, verify = True)
    else:
        response = requests.get(url, verify = cert_path)

    if response.status_code == 200:
        print("   Writing file to: ", full_path)
        with full_path.open("wb") as f:
            f.write(response.content)
        status = 'success'
    else:
        print("---Data not available on", source_str)
        status = "failed"
    
    return status 
    
    
def get_subset_path(full_path):
    '''
    Build the filename for subset of netcdf file
    '''    
    
    # build subset filename
    parts = full_path.name.split(".")
    
    # if third section ends with 'subset', already a subset filename, return
    if(len(parts[3]) > 6 and parts[3][-6:] == 'subset'):
    
        return full_path
    
    # otherwise insert 'subset' into filename
    else:
        parts[3] = parts[3] + "_subset"
        subset_filename = ".".join(parts)
        subset_path = full_path.parent / subset_filename
    
        return subset_path

    
def store_netcdf_subset(full_path, delete_full_file = True):
    '''
    Create and store a subset of netcdf files containing only the variables
    needed for evaluation, keeping or deleting full file as specifed by flag 'delete_full_file'
    - forcing files keep RAINRATE only
    - channel_rt files keep streamflow and nudge only
    '''
    
    # get the variable from the full path
    variable = full_path.name.split(".")[3]
    
    # get the variable and subset path from the full path
    subset_path = get_subset_path(full_path)
    
    # based on variable, set list of data arrays to keep
    if variable == 'forcing':
        var_list = ['RAINRATE']
    else:
        var_list = ['streamflow','nudge','qSfcLatRunoff','qBucket']
        
    # open the dataset
    try:
        ds = xr.open_dataset(full_path)
        
        # get subset
        ds_sub = ds[var_list]

        # write the subset netcdf to disk
        print('   Writing subset netcdf file: ', subset_path)
        ds_sub.to_netcdf(subset_path)
        
        if delete_full_file:
            print('   Deleting full file:', full_path)
            
            # close the full file
            ds.close()        
            
            # delete the full file
            full_path.unlink()        
              
    except:
        print('!!!File unreadable, cannot store subset, deleting full file')
        full_path.unlink()
        
    return
